{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0cc253a-b487-432a-8351-389f52291214",
   "metadata": {},
   "source": [
    "# How neural nets work, dumbed down\n",
    "\n",
    "As I'm learning how neural networks and transformers work, this is my attempt to crystallize my understanding with the simplest possible examples of different concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0fd3a-4b88-47cc-bdbf-2b0bf5e57c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2c7ea-5f77-48d2-a819-9d3a3197a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some pretend training data, a simple line (almost)\n",
    "x = [-4, -2, 0,1,2,3]\n",
    "y = [-8,-4, 1, 2.2, 3.3, 5.3]\n",
    "plt.plot(x, y, 'og')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa5a43d-d40c-46fa-a5de-c7a4b8006127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to create the simplest model, a single \"neuron\"\n",
    "# It will only have two parameters: m and b \n",
    "# It's also the same as a basic linear regression\n",
    "# y = m*x + b\n",
    "# A real neural net would have many, many of these interacting with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43787ce9-28fc-44b0-8979-0420504ba990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize m and b to some random values\n",
    "# m = random.random()\n",
    "# b = random.random()\n",
    "m = -3\n",
    "b = 20\n",
    "print(f'{m=}')\n",
    "print(f'{b=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's initialize some variables to track our progress with every training pass (forward and back propagation)\n",
    "losses = []\n",
    "passes = 0\n",
    "\n",
    "# And let's setup a fig so we can plot the gradient descent \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, 'og')\n",
    "\n",
    "# Calculate y with our initialized m and b params\n",
    "y_pred = [xi * m + b for xi in x]\n",
    "\n",
    "# Plot it\n",
    "ax.plot(x, y_pred, 'r', alpha = 0.05)\n",
    "\n",
    "# And let's start to collect frames for a GIF of the gradient descent\n",
    "frames = []\n",
    "fig.canvas.draw()\n",
    "image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "frames.append(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c09049-934e-43a8-86b2-8fff9fcfa880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop is extremely simple\n",
    "\n",
    "for _ in range(50):\n",
    "    passes +=1\n",
    "\n",
    "    # Iterate through the training data\n",
    "    # We could randomly select but we are keeping it simple\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        # Forward pass to calculate output\n",
    "        # This is just y = mx + b for the current input data\n",
    "        out = m * x[i] + b\n",
    "        \n",
    "        # Calculate the loss and keep track of it \n",
    "        loss = out - y[i]\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Now the magic: backward propagation \n",
    "\n",
    "        # First we pick a step size \n",
    "        # There is a whole art form to picking this right\n",
    "        # Basically it's how far we want to nudge our parameters each pass\n",
    "        # Too large moves them faster but we might skip PAST the optimal value\n",
    "        # Too small takes forever to train\n",
    "        step_size = 0.01\n",
    "\n",
    "        # Next we calculate the gradient of each parameter\n",
    "        # Which is the derivative of how the loss changes compared to how the param changes\n",
    "        # In other words, which direction do we need to move the parameter to minimize the loss\n",
    "        # And how much do we need to move it (is it very sensitive or not)\n",
    "        # We multiply the parameter gradient by the loss due to the chain rule to find the gradient with respect to the loss\n",
    "\n",
    "        # Let's start with the gradient for parameter b\n",
    "        # It's super simple, it's just 1 since it's a constant\n",
    "        # dy/db = 1\n",
    "        # Here's the math with the formula for a derivative        \n",
    "        # b_grad_calc = ((m*x + b+h) -  (m*x + b)) / h \n",
    "                    # =  (m*x - m*x +b-b +h) / h\n",
    "                    # =   h / h = 1\n",
    "        b_grad = 1 * loss \n",
    "\n",
    "        # The gradient of m is also straightforward, it's just x\n",
    "        # dy/dm = x\n",
    "        # Here's the math with the formula for a derivative\n",
    "        # m_grad_calc = (((m+h)x + b) - (mx + b)) / h\n",
    "                    # = (mx +hx +b -mx - b ) / h\n",
    "                    # = hx / x = x\n",
    "        m_grad = x[i] * loss \n",
    "        \n",
    "        # Now we do our backward propagation step\n",
    "        # We adjust each parameter based on the step size and it's linear gradient\n",
    "        # AKA we nudge the parameters in the direction that will decrease our loss\n",
    "        m -= m_grad * step_size\n",
    "        b -= b_grad * step_size\n",
    "\n",
    "        # Optionally can print out variables for debugging        \n",
    "        # print(f'{loss=}')\n",
    "        # print(f'{m=}')\n",
    "        # print(f'{b=}')\n",
    "\n",
    "    # At the end of each pass through all the training data we'll calculate predictions and plot them\n",
    "    # This shows us how close our model is to the real training data\n",
    "    y_pred = [xi * m + b for xi in x]\n",
    "    ax.plot(x, y_pred, 'r', alpha = 1 - np.exp(-0.05 * passes))\n",
    "\n",
    "    # We'll also add the chart to our GIF\n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    frames.append(Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf55666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the GIF\n",
    "frames[0].save('sgd_line4.gif',\n",
    "               save_all=True, append_images=frames[1:], optimize=False, duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how the losses changed over time\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
